{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from scipy.special import expit\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########## DATA HANDLING AND PREPARATION ##########\n",
    "\n",
    "class DataPreparer(Dataset):\n",
    "    def __init__(self, annotation_file, directory, vocabulary, index_map):\n",
    "        self.annotation_file = annotation_file\n",
    "        self.directory = directory\n",
    "        self.audio_visual_content = load_audio_data(annotation_file)\n",
    "        self.index_map = index_map\n",
    "        self.vocabulary = vocabulary\n",
    "        self.data_pairs = link_annotations(directory, vocabulary, index_map)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert (index < self.__len__())\n",
    "        audio_name, text = self.data_pairs[index]\n",
    "        content = torch.Tensor(self.audio_visual_content[audio_name])\n",
    "        content += torch.Tensor(content.size()).random_(0, 2000)/10000\n",
    "        return torch.Tensor(content), torch.Tensor(text)\n",
    "\n",
    "class TestDataLoader(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.audio_content = []\n",
    "        files = os.listdir(data_path)\n",
    "        for file in files:\n",
    "            identifier = file.split('.npy')[0]\n",
    "            content = np.load(os.path.join(data_path, file))\n",
    "            self.audio_content.append((identifier, content))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_content)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.audio_content[index]\n",
    "\n",
    "def build_vocabulary(min_word_frequency):\n",
    "    with open('training_label.json', 'r') as file:\n",
    "        annotations = json.load(file)\n",
    "\n",
    "    word_frequency = {}\n",
    "    for item in annotations:\n",
    "        for sentence in item['caption']:\n",
    "            processed_sentence = re.sub('[.!,;?]]', ' ', sentence).split()\n",
    "            for word in processed_sentence:\n",
    "                word = word.rstrip('.')  # Remove trailing period\n",
    "                word_frequency[word] = word_frequency.get(word, 0) + 1\n",
    "\n",
    "    vocabulary = {}\n",
    "    for word, freq in word_frequency.items():\n",
    "        if freq > min_word_frequency:\n",
    "            vocabulary[word] = freq\n",
    "\n",
    "    token_mapping = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
    "    index_to_word = {index + len(token_mapping): word for index, word in enumerate(vocabulary)}\n",
    "    word_to_index = {word: index + len(token_mapping) for index, word in enumerate(vocabulary)}\n",
    "    index_to_word.update({value: key for key, value in token_mapping.items()})\n",
    "    word_to_index.update({key: value for key, value in token_mapping.items()})\n",
    "\n",
    "    return index_to_word, word_to_index, vocabulary\n",
    "\n",
    "def split_and_tokenize_sentence(sentence, vocabulary, word_to_index):\n",
    "    sentence = re.sub(r'[.!,;?]', ' ', sentence).split()\n",
    "    tokenized_sentence = [word_to_index.get(word, 3) for word in sentence]  # 3 is for <UNK>\n",
    "    tokenized_sentence = [1] + tokenized_sentence + [2]  # Add <SOS> and <EOS>\n",
    "    return tokenized_sentence\n",
    "\n",
    "def link_annotations(annotation_file, vocabulary, word_to_index):\n",
    "    linked_data = []\n",
    "    with open(annotation_file, 'r') as file:\n",
    "        annotations = json.load(file)\n",
    "    for item in annotations:\n",
    "        for sentence in item['caption']:\n",
    "            processed_sentence = split_and_tokenize_sentence(sentence, vocabulary, word_to_index)\n",
    "            linked_data.append((item['id'], processed_sentence))\n",
    "    return linked_data\n",
    "\n",
    "def load_audio_data(directory):\n",
    "    audio_data_map = {}\n",
    "    files = os.listdir(directory)\n",
    "    for file in files:\n",
    "        content = np.load(os.path.join(directory, file))\n",
    "        audio_data_map[file.split('.npy')[0]] = content\n",
    "    return audio_data_map\n",
    "\n",
    "\n",
    "def train_network(model, current_epoch, loader, loss_function):\n",
    "    model.train()\n",
    "    print(current_epoch)\n",
    "    model.cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    for batch_index, (audio_features, labels, sequence_lengths) in enumerate(loader):\n",
    "        audio_features, labels = audio_features.cuda(), labels.cuda()\n",
    "        audio_features, labels = Variable(audio_features), Variable(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        seq_log_probs, seq_predictions = model(audio_features, target_sentences=labels, mode='train', training_steps=current_epoch)\n",
    "\n",
    "        labels = labels[:, 1:]  \n",
    "        loss = compute_loss(seq_log_probs, labels, sequence_lengths, loss_function)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {current_epoch}, Loss: {loss.item():.3f}')\n",
    "\n",
    "def evaluate_network(loader, model):\n",
    "    model.eval()\n",
    "    for batch_index, (audio_features, labels, sequence_lengths) in enumerate(loader):\n",
    "        audio_features, labels = audio_features.cuda(), labels.cuda()\n",
    "        audio_features, labels = Variable(audio_features), Variable(labels)\n",
    "\n",
    "        seq_log_probs, seq_predictions = model(audio_features, mode='inference')\n",
    "        return seq_predictions[:3], labels[:3]  \n",
    "\n",
    "def minibatch_collate(batch_data):\n",
    "    batch_data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    audio_data, text_sequences = zip(*batch_data)\n",
    "    audio_tensor = torch.stack(audio_data, 0)\n",
    "\n",
    "    sequence_lengths = [len(sequence) for sequence in text_sequences]\n",
    "    max_length = max(sequence_lengths)\n",
    "    padded_sequences = torch.zeros(len(text_sequences), max_length).long()\n",
    "    for i, sequence in enumerate(text_sequences):\n",
    "        end = sequence_lengths[i]\n",
    "        padded_sequences[i, :end] = torch.LongTensor(sequence[:end])\n",
    "\n",
    "    return audio_tensor, padded_sequences, sequence_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    annotation_file = 'training_label.json'\n",
    "    training_data_dir = 'training_data/feat'\n",
    "    index_to_word, word_to_index, vocabulary = build_vocabulary(4)\n",
    "    train_dataset = DataPreparer(annotation_file, training_data_dir, vocabulary, word_to_index)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=5, shuffle=True, num_workers=8, collate_fn=minibatch_collate)\n",
    "    \n",
    "    testing_data_dir = 'testing_data/feat'\n",
    "    testing_annotation_file = 'testing_label.json'\n",
    "    test_dataset = DataPreparer(testing_annotation_file, testing_data_dir, vocabulary, word_to_index)\n",
    "    test_dataloader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=True, num_workers=8, collate_fn=minibatch_collate)\n",
    "   \n",
    "    epochs_n = 100\n",
    "    model_save_location = 'model'\n",
    "    \n",
    "    if not os.path.exists(model_save_location):\n",
    "        os.makedirs(model_save_location)\n",
    "    \n",
    "    with open(os.path.join(model_save_location, 'i2wData.pickle'), 'wb') as file:\n",
    "         pickle.dump(index_to_word, file)\n",
    "    \n",
    "    vocab_size = len(index_to_word) + 4 \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    encoder = models.EncoderNet()  \n",
    "    decoder = models.DecoderNet(512, vocab_size, vocab_size, 1024, 0.3) \n",
    "    model = models.ModelMain(encoder=encoder, decoder=decoder) \n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs_n):\n",
    "        train_network(model, epoch + 1, train_dataloader, loss_fn)\n",
    "        evaluate_network(test_dataloader, model)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    torch.save(model.state_dict(), os.path.join(model_save_location, 'model0.pth'))\n",
    "    print(f\"Training completed in {(end_time - start_time):.3f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Epoch:1 & loss:3.494\n",
      "2\n",
      "Epoch:2 & loss:3.219\n",
      "3\n",
      "Epoch:3 & loss:2.475\n",
      "4\n",
      "Epoch:4 & loss:3.842\n",
      "5\n",
      "Epoch:5 & loss:3.28\n",
      "6\n",
      "Epoch:6 & loss:0.891\n",
      "7\n",
      "Epoch:7 & loss:1.65\n",
      "8\n",
      "Epoch:8 & loss:2.606\n",
      "9\n",
      "Epoch:9 & loss:2.885\n",
      "10\n",
      "Epoch:10 & loss:1.941\n",
      "11\n",
      "Epoch:11 & loss:1.094\n",
      "12\n",
      "Epoch:12 & loss:2.179\n",
      "13\n",
      "Epoch:13 & loss:1.769\n",
      "14\n",
      "Epoch:14 & loss:2.679\n",
      "15\n",
      "Epoch:15 & loss:1.698\n",
      "16\n",
      "Epoch:16 & loss:1.944\n",
      "17\n",
      "Epoch:17 & loss:2.57\n",
      "18\n",
      "Epoch:18 & loss:2.761\n",
      "19\n",
      "Epoch:19 & loss:1.889\n",
      "20\n",
      "Epoch:20 & loss:1.711\n",
      "21\n",
      "Epoch:21 & loss:1.65\n",
      "22\n",
      "Epoch:22 & loss:1.566\n",
      "23\n",
      "Epoch:23 & loss:1.873\n",
      "24\n",
      "Epoch:24 & loss:1.406\n",
      "25\n",
      "Epoch:25 & loss:1.588\n",
      "26\n",
      "Epoch:26 & loss:1.398\n",
      "27\n",
      "Epoch:27 & loss:2.641\n",
      "28\n",
      "Epoch:28 & loss:2.962\n",
      "29\n",
      "Epoch:29 & loss:2.436\n",
      "30\n",
      "Epoch:30 & loss:1.687\n",
      "31\n",
      "Epoch:31 & loss:3.341\n",
      "32\n",
      "Epoch:32 & loss:2.022\n",
      "33\n",
      "Epoch:33 & loss:1.766\n",
      "34\n",
      "Epoch:34 & loss:2.221\n",
      "35\n",
      "Epoch:35 & loss:2.354\n",
      "36\n",
      "Epoch:36 & loss:2.369\n",
      "37\n",
      "Epoch:37 & loss:2.006\n",
      "38\n",
      "Epoch:38 & loss:2.966\n",
      "39\n",
      "Epoch:39 & loss:1.944\n",
      "40\n",
      "Epoch:40 & loss:2.854\n",
      "41\n",
      "Epoch:41 & loss:1.965\n",
      "42\n",
      "Epoch:42 & loss:1.885\n",
      "43\n",
      "Epoch:43 & loss:1.634\n",
      "44\n",
      "Epoch:44 & loss:2.608\n",
      "45\n",
      "Epoch:45 & loss:2.905\n",
      "46\n",
      "Epoch:46 & loss:1.525\n",
      "47\n",
      "Epoch:47 & loss:1.825\n",
      "48\n",
      "Epoch:48 & loss:0.855\n",
      "49\n",
      "Epoch:49 & loss:2.394\n",
      "50\n",
      "Epoch:50 & loss:1.872\n",
      "51\n",
      "Epoch:51 & loss:2.468\n",
      "52\n",
      "Epoch:52 & loss:2.365\n",
      "53\n",
      "Epoch:53 & loss:1.666\n",
      "54\n",
      "Epoch:54 & loss:0.831\n",
      "55\n",
      "Epoch:55 & loss:2.819\n",
      "56\n",
      "Epoch:56 & loss:1.837\n",
      "57\n",
      "Epoch:57 & loss:2.785\n",
      "58\n",
      "Epoch:58 & loss:2.498\n",
      "59\n",
      "Epoch:59 & loss:1.706\n",
      "60\n",
      "Epoch:60 & loss:1.308\n",
      "61\n",
      "Epoch:61 & loss:1.968\n",
      "62\n",
      "Epoch:62 & loss:3.002\n",
      "63\n",
      "Epoch:63 & loss:2.43\n",
      "64\n",
      "Epoch:64 & loss:1.604\n",
      "65\n",
      "Epoch:65 & loss:1.32\n",
      "66\n",
      "Epoch:66 & loss:2.024\n",
      "67\n",
      "Epoch:67 & loss:1.44\n",
      "68\n",
      "Epoch:68 & loss:1.973\n",
      "69\n",
      "Epoch:69 & loss:2.67\n",
      "70\n",
      "Epoch:70 & loss:2.868\n",
      "71\n",
      "Epoch:71 & loss:1.846\n",
      "72\n",
      "Epoch:72 & loss:1.798\n",
      "73\n",
      "Epoch:73 & loss:2.16\n",
      "74\n",
      "Epoch:74 & loss:1.839\n",
      "75\n",
      "Epoch:75 & loss:2.361\n",
      "76\n",
      "Epoch:76 & loss:1.941\n",
      "77\n",
      "Epoch:77 & loss:1.477\n",
      "78\n",
      "Epoch:78 & loss:2.338\n",
      "79\n",
      "Epoch:79 & loss:1.354\n",
      "80\n",
      "Epoch:80 & loss:2.191\n",
      "81\n",
      "Epoch:81 & loss:1.159\n",
      "82\n",
      "Epoch:82 & loss:1.468\n",
      "83\n",
      "Epoch:83 & loss:0.539\n",
      "84\n",
      "Epoch:84 & loss:1.414\n",
      "85\n",
      "Epoch:85 & loss:2.471\n",
      "86\n",
      "Epoch:86 & loss:1.788\n",
      "87\n",
      "Epoch:87 & loss:1.936\n",
      "88\n",
      "Epoch:88 & loss:0.726\n",
      "89\n",
      "Epoch:89 & loss:2.532\n",
      "90\n",
      "Epoch:90 & loss:2.327\n",
      "91\n",
      "Epoch:91 & loss:1.261\n",
      "92\n",
      "Epoch:92 & loss:1.641\n",
      "93\n",
      "Epoch:93 & loss:2.141\n",
      "94\n",
      "Epoch:94 & loss:1.995\n",
      "95\n",
      "Epoch:95 & loss:1.236\n",
      "96\n",
      "Epoch:96 & loss:1.997\n",
      "97\n",
      "Epoch:97 & loss:1.108\n",
      "98\n",
      "Epoch:98 & loss:2.872\n",
      "99\n",
      "Epoch:99 & loss:0.851\n",
      "100\n",
      "Epoch:100 & loss:2.159\n",
      "Training finished test  elapsed time:  12522.884 seconds. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
